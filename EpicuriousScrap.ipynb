{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "from urllib.request import Request,urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.epicurious.com/search?content=recipe&page='\n",
    "prefix_url = 'https://www.epicurious.com'\n",
    "\n",
    "recipe_titles = []\n",
    "recipe_img_links = []\n",
    "recipe_ingredients = []\n",
    "recipe_instructions = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 215 scraping started!!!\n",
      "Page 215 Scraped!!!\n",
      "Page 216 scraping started!!!\n",
      "Page 216 Scraped!!!\n",
      "Page 217 scraping started!!!\n",
      "Page 217 Scraped!!!\n",
      "Page 218 scraping started!!!\n",
      "Page 218 Scraped!!!\n",
      "Page 219 scraping started!!!\n",
      "Page 219 Scraped!!!\n",
      "Page 220 scraping started!!!\n",
      "Page 220 Scraped!!!\n",
      "Page 221 scraping started!!!\n",
      "Page 221 Scraped!!!\n",
      "Page 222 scraping started!!!\n",
      "Page 222 Scraped!!!\n",
      "Page 223 scraping started!!!\n",
      "Page 223 Scraped!!!\n",
      "Page 224 scraping started!!!\n",
      "Page 224 Scraped!!!\n",
      "Page 225 scraping started!!!\n",
      "Page 225 Scraped!!!\n",
      "Page 226 scraping started!!!\n",
      "Page 226 Scraped!!!\n",
      "Page 227 scraping started!!!\n",
      "Page 227 Scraped!!!\n",
      "Page 228 scraping started!!!\n",
      "Page 228 Scraped!!!\n",
      "Page 229 scraping started!!!\n",
      "Page 229 Scraped!!!\n",
      "Page 230 scraping started!!!\n",
      "Page 230 Scraped!!!\n",
      "Page 231 scraping started!!!\n",
      "Page 231 Scraped!!!\n",
      "Page 232 scraping started!!!\n",
      "Page 232 Scraped!!!\n",
      "Page 233 scraping started!!!\n",
      "Page 233 Scraped!!!\n",
      "Page 234 scraping started!!!\n",
      "Page 234 Scraped!!!\n",
      "Page 235 scraping started!!!\n",
      "Page 235 Scraped!!!\n",
      "Page 236 scraping started!!!\n",
      "Page 236 Scraped!!!\n",
      "Page 237 scraping started!!!\n",
      "Page 237 Scraped!!!\n",
      "Page 238 scraping started!!!\n",
      "Page 238 Scraped!!!\n",
      "Page 239 scraping started!!!\n",
      "Page 239 Scraped!!!\n",
      "Page 240 scraping started!!!\n",
      "Page 240 Scraped!!!\n",
      "Page 241 scraping started!!!\n",
      "Page 241 Scraped!!!\n",
      "Page 242 scraping started!!!\n",
      "Page 242 Scraped!!!\n",
      "Page 243 scraping started!!!\n",
      "Page 243 Scraped!!!\n",
      "Page 244 scraping started!!!\n",
      "Page 244 Scraped!!!\n",
      "Page 245 scraping started!!!\n",
      "Page 245 Scraped!!!\n",
      "Page 246 scraping started!!!\n",
      "Page 246 Scraped!!!\n",
      "Page 247 scraping started!!!\n",
      "Page 247 Scraped!!!\n",
      "Page 248 scraping started!!!\n",
      "Page 248 Scraped!!!\n",
      "Page 249 scraping started!!!\n",
      "Page 249 Scraped!!!\n",
      "Page 250 scraping started!!!\n",
      "Page 250 Scraped!!!\n",
      "Page 251 scraping started!!!\n",
      "Page 251 Scraped!!!\n",
      "Page 252 scraping started!!!\n",
      "Page 252 Scraped!!!\n",
      "Page 253 scraping started!!!\n",
      "Page 253 Scraped!!!\n",
      "Page 254 scraping started!!!\n",
      "Page 254 Scraped!!!\n",
      "Page 255 scraping started!!!\n",
      "Page 255 Scraped!!!\n",
      "Page 256 scraping started!!!\n",
      "Page 256 Scraped!!!\n",
      "Page 257 scraping started!!!\n",
      "Page 257 Scraped!!!\n",
      "Page 258 scraping started!!!\n",
      "Page 258 Scraped!!!\n",
      "Page 259 scraping started!!!\n",
      "Page 259 Scraped!!!\n",
      "Page 260 scraping started!!!\n",
      "Page 260 Scraped!!!\n",
      "Page 261 scraping started!!!\n",
      "Page 261 Scraped!!!\n",
      "Page 262 scraping started!!!\n",
      "Page 262 Scraped!!!\n",
      "Page 263 scraping started!!!\n",
      "Page 263 Scraped!!!\n",
      "Page 264 scraping started!!!\n",
      "Page 264 Scraped!!!\n",
      "Page 265 scraping started!!!\n",
      "Page 265 Scraped!!!\n",
      "Page 266 scraping started!!!\n",
      "Page 266 Scraped!!!\n",
      "Page 267 scraping started!!!\n",
      "Page 267 Scraped!!!\n",
      "Page 268 scraping started!!!\n",
      "Page 268 Scraped!!!\n",
      "Page 269 scraping started!!!\n",
      "Page 269 Scraped!!!\n",
      "Page 270 scraping started!!!\n",
      "Page 270 Scraped!!!\n",
      "Page 271 scraping started!!!\n",
      "Page 271 Scraped!!!\n",
      "Page 272 scraping started!!!\n",
      "Page 272 Scraped!!!\n",
      "Page 273 scraping started!!!\n",
      "Page 273 Scraped!!!\n",
      "Page 274 scraping started!!!\n",
      "Page 274 Scraped!!!\n",
      "Page 275 scraping started!!!\n",
      "Page 275 Scraped!!!\n",
      "Page 276 scraping started!!!\n",
      "Page 276 Scraped!!!\n",
      "Page 277 scraping started!!!\n",
      "Page 277 Scraped!!!\n",
      "Page 278 scraping started!!!\n",
      "Page 278 Scraped!!!\n",
      "Page 279 scraping started!!!\n",
      "Page 279 Scraped!!!\n",
      "Page 280 scraping started!!!\n",
      "Page 280 Scraped!!!\n",
      "Page 281 scraping started!!!\n",
      "Page 281 Scraped!!!\n",
      "Page 282 scraping started!!!\n",
      "Page 282 Scraped!!!\n",
      "Page 283 scraping started!!!\n",
      "Page 283 Scraped!!!\n",
      "Page 284 scraping started!!!\n",
      "Page 284 Scraped!!!\n",
      "Page 285 scraping started!!!\n",
      "Page 285 Scraped!!!\n",
      "Page 286 scraping started!!!\n",
      "Page 286 Scraped!!!\n",
      "Page 287 scraping started!!!\n",
      "Page 287 Scraped!!!\n",
      "Page 288 scraping started!!!\n",
      "Page 288 Scraped!!!\n",
      "Page 289 scraping started!!!\n",
      "Page 289 Scraped!!!\n",
      "Page 290 scraping started!!!\n",
      "Page 290 Scraped!!!\n",
      "Page 291 scraping started!!!\n",
      "Page 291 Scraped!!!\n",
      "Page 292 scraping started!!!\n",
      "Page 292 Scraped!!!\n",
      "Page 293 scraping started!!!\n",
      "Page 293 Scraped!!!\n",
      "Page 294 scraping started!!!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-40753aa5ff28>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_url\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'User-Agent'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Mozilla/5.0'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mwebpage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mpage_soup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwebpage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"html.parser\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    524\u001b[0m             \u001b[0mreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m         \u001b[1;31m# post-process response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    542\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[1;32m--> 544\u001b[1;33m                                   '_open', req)\n\u001b[0m\u001b[0;32m    545\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    502\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 504\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    505\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1359\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[1;32m-> 1361\u001b[1;33m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[0m\u001b[0;32m   1362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m         \u001b[0mhttps_request\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAbstractHTTPHandler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_request_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mdo_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# timeout error\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1322\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m             \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1329\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1331\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1332\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    295\u001b[0m         \u001b[1;31m# read until we get a non-100 response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 297\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    298\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"status line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1007\u001b[0m                   \u001b[1;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1008\u001b[0m                   self.__class__)\n\u001b[1;32m-> 1009\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1010\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1011\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m    869\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Read on closed or unwrapped SSL socket.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    872\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m    629\u001b[0m         \"\"\"\n\u001b[0;32m    630\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 631\u001b[1;33m             \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    632\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m             \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loop through the pages\n",
    "cur_count=5228\n",
    "count=1\n",
    "for i in range(294,500):  # 2253\n",
    "    print(\"Page \" + str(i) + \" scraping started!!!\")\n",
    "    \n",
    "    url = base_url + str(i)\n",
    "    req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    webpage = urlopen(req).read()\n",
    "    page_soup = BeautifulSoup(webpage, \"html.parser\")\n",
    "    \n",
    "    recipe_links_on_page = []\n",
    "    \n",
    "    recipe_links = page_soup.find_all('h4', {'class': 'hed'})\n",
    "#     print(recipe_links)\n",
    "    \n",
    "\n",
    "    for link in recipe_links:\n",
    "        recipe_links_on_page.append(prefix_url + link.find('a')['href'])\n",
    "        \n",
    "        # Convert the Beautiful Soup object to string\n",
    "        link_html = str(link)  \n",
    "        \n",
    "        # Use BeautifulSoup on the HTML string\n",
    "        soup = BeautifulSoup(link_html, 'html.parser')  \n",
    "\n",
    "        # Finding the <a> tag within <h4> tag\n",
    "        a_tag = soup.find('a')\n",
    "\n",
    "        # Extracting text from the <a> tag\n",
    "        if a_tag:\n",
    "            recipe_name = a_tag.text\n",
    "            recipe_titles.append(recipe_name)\n",
    "        else:\n",
    "            # If <a> tag not found, add \"N/A\"\n",
    "            recipe_titles.append(\"N/A\")  \n",
    "\n",
    "# print(recipe_links_on_page)\n",
    "    for recipe_url in recipe_links_on_page:\n",
    "        req_recipe = Request(recipe_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        webpage_recipe = urlopen(req_recipe).read()\n",
    "        page_soup_recipe = BeautifulSoup(webpage_recipe, \"html.parser\")\n",
    "        \n",
    "        # Ingredients\n",
    "        ingredients = page_soup_recipe.find('div', class_='List-iSNGTT ljAYJm').find_all('div')\n",
    "        temp_ingredients = [ingredient.text.strip() for ingredient in ingredients]\n",
    "        recipe_ingredients.append(temp_ingredients)\n",
    "#         print(temp_ingredients)\n",
    "\n",
    "\n",
    "        # Recipe Instructions\n",
    "        instructions = page_soup_recipe.find('div', class_='InstructionsWrapper-hZXqPx RmryN').find('ol').find('li').find_all('p')\n",
    "        temp_instructions = [instruction.text.strip() for instruction in instructions]\n",
    "        recipe_instructions.append(temp_instructions)\n",
    "#         print(recipe_intructions)\n",
    "\n",
    "        # Creating JSON object for individual recipe\n",
    "        entry = {\n",
    "            'Recipe_number': cur_count,\n",
    "            'Recipe_name': recipe_titles[count-1],\n",
    "            'Recipe_ingredients': temp_ingredients,\n",
    "            'Recipe_instructions': temp_instructions,\n",
    "#             'Recipe_img_link': temp_img\n",
    "        }\n",
    "\n",
    "        # Append to JSON file\n",
    "        with open(\"dataset_epi.json\", mode='a', encoding='utf-8') as feedsjson:\n",
    "            json.dump(entry, feedsjson)\n",
    "            feedsjson.write('\\n')\n",
    "        cur_count+=1\n",
    "        count+=1\n",
    "        \n",
    "#         print(\"Recipe \" + str(len(recipe_titles)) + \" Scrapped\")\n",
    "    print(\"Page \"+str(i)+\" Scraped!!!\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# Define base_url, prefix_url, and other variables here\n",
    "\n",
    "count = 1\n",
    "\n",
    "# Create a retry session\n",
    "retries = Retry(total=3, backoff_factor=0.5, status_forcelist=[502, 503, 504])\n",
    "session = requests.Session()\n",
    "session.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "session.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "\n",
    "for i in range(1, 2254):  # 91-2254\n",
    "    print(\"Page \" + str(i) + \" scraping started!!!\")\n",
    "    \n",
    "    url = base_url + str(i)\n",
    "    \n",
    "    try:\n",
    "        response = session.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=30)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        recipe_links_on_page = []\n",
    "        recipe_links = soup.find_all('h4', {'class': 'hed'})\n",
    "\n",
    "        for link in recipe_links:\n",
    "            recipe_links_on_page.append(prefix_url + link.find('a')['href'])\n",
    "            recipe_name = link.find('a').text.strip()\n",
    "            recipe_titles.append(recipe_name if recipe_name else \"N/A\")\n",
    "\n",
    "        for recipe_url in recipe_links_on_page:\n",
    "            try:\n",
    "                response_recipe = session.get(recipe_url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=30)\n",
    "                soup_recipe = BeautifulSoup(response_recipe.content, \"html.parser\")\n",
    "                \n",
    "                # Ingredients\n",
    "                ingredients = soup_recipe.find('div', class_='List-iSNGTT ljAYJm').find_all('div')\n",
    "                temp_ingredients = [ingredient.text.strip() for ingredient in ingredients]\n",
    "                recipe_ingredients.append(temp_ingredients)\n",
    "\n",
    "                # Recipe Instructions\n",
    "                instructions = soup_recipe.find('div', class_='InstructionsWrapper-hZXqPx RmryN').find('ol').find('li').find_all('p')\n",
    "                temp_instructions = [instruction.text.strip() for instruction in instructions]\n",
    "                recipe_instructions.append(temp_instructions)\n",
    "\n",
    "                # Creating JSON object for individual recipe\n",
    "                entry = {\n",
    "                    'Recipe_number': count,\n",
    "                    'Recipe_name': recipe_titles[count - 1],\n",
    "                    'Recipe_ingredients': temp_ingredients,\n",
    "                    'Recipe_instructions': temp_instructions,\n",
    "                }\n",
    "\n",
    "                # Append to JSON file\n",
    "                with open(\"dataset_epi.json\", mode='a', encoding='utf-8') as feedsjson:\n",
    "                    json.dump(entry, feedsjson)\n",
    "                    feedsjson.write('\\n')\n",
    "                count += 1\n",
    "                \n",
    "            except (requests.HTTPError, requests.ConnectionError, requests.Timeout) as e:\n",
    "                print(f\"Error fetching recipe URL: {recipe_url}. Error: {e}\")\n",
    "            \n",
    "        print(\"Page \" + str(i) + \" Scraped!!!\")\n",
    "\n",
    "    except (requests.HTTPError, requests.ConnectionError, requests.Timeout) as e:\n",
    "        print(f\"Error fetching page URL: {url}. Error: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read json data\n",
    "\n",
    "import json\n",
    "\n",
    "data = []\n",
    "with open('dataset_epi.json', 'r', encoding='utf-8') as json_file:\n",
    "    for line in json_file:\n",
    "        try:\n",
    "            item = json.loads(line)\n",
    "            data.append(item)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {e}\")\n",
    "\n",
    "# Now 'data' contains all the valid JSON objects\n",
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
